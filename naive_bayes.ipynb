{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (22MF10006)\n",
    "---\n",
    "---\n",
    "# Subtask 1: Mathematical Understanding\n",
    "\n",
    "- Naive bayes is a probabilistic classification algorithm which is based on Bayes' theorem with the 'naive' assumption that the input features are independent of each other. It is a supervised machine learning algorithm.\n",
    "\n",
    "### Principles and Assumptions:\n",
    "\n",
    "- Naive Bayes is based on the fundamental principles of Bayes' theorem which is used to calculate the probability of an event happening provided that another event has already occurred.\n",
    "\n",
    "- It assumes that the features are independent of each other and the presence or absence of another feature does not affect any other feature in any manner possible.\n",
    "\n",
    "### Mathematical Explanation:\n",
    "\n",
    "- Bayes' Theorem : It is mathematically expressed as following:\n",
    "\n",
    "            P(class | features) = (P(features | class) * P(class)) / P(features)\n",
    "\n",
    "     - where P(class | features) represents the probability of class label given the observed features\n",
    "     - P(features | class) is for the probability of the features given the class label\n",
    "     - P(class) is the prior probability of the class label (here prior probability refers to the possibility of an event provided there is a finite no of outcomes and each one is equally likely to occur)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- since P(features) will remain constant for all values of our dataset, hence we directly write posterior probability as being directly proportional to the product of conditional probability of each feature belonging to the given class label.\n",
    "\n",
    "### How model learns and makes predictions:\n",
    "\n",
    "- During the training phase of the model, likelihood probability or P(features | class) is calculated which represents the probability of observing the features given a specific class label. For features which are categorical, it is calculated as the frequency of each feature within the each class label as we have made the assumption that features are independent. The math has been shown below\n",
    "\n",
    "       P(features | class) = P(feature_1 | class) * P(feature_2 | class) * P(feature_3 | class)...P(feature_n | class)\n",
    "\n",
    "for continous features, the likelihood probability is modeled usually using probability density functions, for example in gaussian naive bayes model we assume that each feature assumes gaussian normal distribution.\n",
    "\n",
    "- The prior probability or P(class) represents the probability of a particular class label in the particular set of data. It is calculated using the no of times that particular label occurs in the data_set.\n",
    "\n",
    "- During the training phase, the model is provided with data which has labeled class labels and their corresponding features. The algorithm uses the data to calculate the priori and likelihood probabilities.\n",
    "\n",
    "- To make predictions for the test data, the model uses Bayes' Theorem to calculate P(class | features) or the posterior probabilities for each class label. The class label with highest posteriori probability is considered the predicted class for the particular set of input features.\n",
    "\n",
    "- In the given data_Set we have numerical features hence we can use gaussian naive bayes model which is a variant of naive bayes. In this model we assume that the features follow a gaussian (Normal) distribution within each class.\n",
    "\n",
    "- for estimating likelihood probabilities, we estimate the mean (μ) and standard deviation (σ) of each numerical feature within that class which is used to model the gaussian distribution of each feature of each class. \n",
    "for a class label(c), and for a numerical feature(x) the likelihood probability, P(x | c) can be found out using probability density function of the gaussian distribution, \n",
    "\n",
    "              P(x | c) = (1 / (√(2π) * σ_c)) * exp(-(x - μ_c)^2 / (2 * σ_c^2))\n",
    "\n",
    "        where μ_c is the mean of feature x for class c\n",
    "        σ_c is the standard deviation of feature x for class c.\n",
    "        π is pi (mathematical constant)\n",
    "        exp() is the exponential function\n",
    "\n",
    "- as in our classic naive bayes algorithm, here also we estimate prior probabilities P(c) for each class c and to make predictions we calculate the posterior probabilities P(c | x) for each class label c using bayes' theorem and the estimated likelihood and prior probabilities\n",
    "\n",
    "       P(c | x) = (P(x | c) * P(c)) / P(x)\n",
    "\n",
    "       where P(c | x) is the posterior probability of class c given the numerical features x.\n",
    "       P(x | c) is the probability of observing features x given class c (calculated using the Gaussian distribution).\n",
    "       P(c) is the prior probability of class c\n",
    "       P(x) is the probability of observing the numerical features x (a normalization factor)\n",
    "\n",
    "- The class label with the highest posterior probability is considered the predicted class for the given set of numerical features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Subtask 2: Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "\n",
    "    def fit(self, X, y, alpha=1.0, class_priors=None):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # Calculate prior probabilities\n",
    "        if class_priors is not None:\n",
    "            self._priors = np.array(class_priors)\n",
    "        else:\n",
    "            self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "            for idx, c in enumerate(self._classes):\n",
    "                self._priors[idx] = np.mean(y == c)\n",
    "\n",
    "        # Laplace smoothing parameter\n",
    "        self._alpha = alpha\n",
    "\n",
    "        # calculate mean, var for each class\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[idx, :] = X_c.mean(axis=0)\n",
    "            self._var[idx, :] = X_c.var(axis=0)\n",
    "\n",
    "        # Add alpha to variances for Laplace smoothing\n",
    "        self._var += alpha\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "\n",
    "        # calculate posterior probability for each class\n",
    "        for idx, c in enumerate(self._classes):\n",
    "            prior = np.log(self._priors[idx])\n",
    "            posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # return class with the highest posterior\n",
    "        return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _pdf(self, class_idx, x):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = self._var[class_idx]\n",
    "        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n",
    "        denominator = np.sqrt(2 * np.pi * var)\n",
    "        return numerator / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyGaussianNaiveBayes:\n",
    "    \n",
    "    def __init__(self, smoothing=1.0):\n",
    "        self.smoothing = smoothing\n",
    "        self.means = None\n",
    "        self.variances = None\n",
    "        self.class_priors = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        samples_count, features_count = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        classes_count = len(self.classes)\n",
    "\n",
    "        # class_priors stores prior probabilities, means stores mean for a particular feature provided a given class, similarly, we have variance too\n",
    "        self.class_priors = np.zeros(classes_count)\n",
    "        self.means = np.zeros((classes_count, features_count))\n",
    "        self.variances = np.zeros((classes_count, features_count))\n",
    "\n",
    "        # Here we calculate prior probabilities of classes and parameters for each feature\n",
    "        for i, target_class in enumerate(self.classes):\n",
    "            X_c = X[y == target_class]\n",
    "            self.class_priors[i] = (len(X_c) + self.smoothing) / (samples_count + classes_count * self.smoothing)\n",
    "            self.means[i] = X_c.mean(axis=0)\n",
    "            self.variances[i] = X_c.var(axis=0)\n",
    "\n",
    "    def _likelihood(self, X, i):\n",
    "        mean = self.means[i]\n",
    "        variance = self.variances[i]\n",
    "        return np.exp(-0.5 * ((X - mean) ** 2) / (variance + 1e-8)) / np.sqrt(2 * np.pi * variance + 1e-8)\n",
    "\n",
    "    def _predict_one_sample(self, x):\n",
    "        posteriors = []\n",
    "        for i, c in enumerate(self.classes):\n",
    "            likelihood = self._likelihood(np.expand_dims(x, axis=0), i).prod(axis=1)\n",
    "            posterior = self.class_priors[i] * likelihood\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict_one_sample(x) for x in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 2\n",
      "Best accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('ds2_train.csv')\n",
    "X_train = train[['x_1', 'x_2']].to_numpy()\n",
    "y_train = train['y'].to_numpy()\n",
    "\n",
    "test = pd.read_csv('ds2_test.csv')\n",
    "X_test = test[['x_1', 'x_2']].to_numpy()\n",
    "y_test = test['y'].to_numpy()\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct_predictions = 0\n",
    "    total_samples = len(y_true)\n",
    "\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label == pred_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_alpha = None\n",
    "alphas = [ 2, 10.0, 100.0, 0.0001, 0.02, 2, 1000]  \n",
    "\n",
    "for alpha in alphas:\n",
    "    model = MyGaussianNaiveBayes(smoothing=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(\"Best alpha:\", best_alpha)\n",
    "print(\"Best accuracy:\", best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
