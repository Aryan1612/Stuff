{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (22MF10006)\n",
    "---\n",
    "---\n",
    "        Note: ALL THE SUBTASKS ARE IN THIS SINGLE PYTHON NOTEBOOK ONLY\n",
    "# Subtask 1: Mathematical Understanding\n",
    "\n",
    "- Logistic Regression is a machine learning model which is used for binary classification problems. It is a classification algorithm and not a regression algorithm. This model applies the sigmoid function or the logistic function to a linear combination of input features which is then mapped to any value from 0 to 1.  It is a supervised learning algorithm.  \n",
    "    \n",
    "### Principles and Assumptions:\n",
    "\n",
    "- It is designed for binary classification where the output or the target variable can take only two values (eg 0 or 1), for example predicting whether a student will pass his end-semester examinations for which the input features can be number of hours studied and no of hours slept.\n",
    "\n",
    "- It assumes that the log-odds (also called logit, where logit(p) = log(p/1-p)) of the target variable or the outcome are a linear combination of the input features or parameters\n",
    "\n",
    "- It also assumes that the errors in predicting log odds are independent of each other\n",
    "\n",
    "- The independent variables or the input features should not be highly correlated to each other.\n",
    "\n",
    "- For better results large data set should be taken.\n",
    "\n",
    "### Mathematical Explanation:\n",
    "\n",
    "- The Sigmoid function, is used to transform the linear combination of input features and their weight or coeffiecients into a value between 0 and 1. It is defined as follows:\n",
    "\n",
    "        sigma(z) = 1/(1+exp(-z))\n",
    "\n",
    "        where sigma(z) is the output between 0 to 1,\n",
    "        exp is the exponential function,\n",
    "        z is the linear combination of the input features and their respective coefficients or weights\n",
    "\n",
    "- The logistic regression establishes a relationship between the features and the log-odds (logit) of the target variable.\n",
    "\n",
    "        logit(y) = log( p(y=1)/ (1- p(y=1) ) )\n",
    "\n",
    "        where p(y=1) is the probability that target variable y belongs to class 1 or the Positive class\n",
    "\n",
    "        logit(y) = w0+ w1x1+ w2x2 + w3x3 + .....wnxn\n",
    "\n",
    "        where w0, w1, w2, w3 ...wn represents the coefficients or weights associated with each feature\n",
    "        x1, x2, x3 ... xn represents the input features\n",
    "        logit(y) is the logit of the target variable\n",
    "\n",
    "\n",
    "### How model learns and makes predictions:\n",
    "\n",
    "- Initially the model starts with random values or zero values for all its coefficients w0, w1, w2....wn.\n",
    "\n",
    "- In order to determine how good our model is performing we minimise a function called loss function which is defined as\n",
    "\n",
    "        loss function or cost function= - (1\\n)sum( ylog(y^) + (1-y) log(1-y^) )\n",
    "        where y^ is the predicted output and y is the actual output and we sum it over all input features\n",
    "\n",
    "\n",
    "- for every data point in the training set, logit z is calculated using current coefficients and then we use the sigmoid function to obtain sigma(z) which is use to decide if the data point belongs to class 1 or positive class.\n",
    "\n",
    "- Once we obtain the predicted probabilities, we then compare to them to the actual target values by calculating the prediction errors, which represent the difference between the predicted probabilities and the actual target values.\n",
    "\n",
    "- to minimise the loss function we update our coefficients using various iterative optimaztion algorithms such as gradient descent which computes the gradient of the loss function with respect to each coefficient and adjusts them so that it reduces loss. \n",
    "\n",
    "- The above steps goes on until we reach a certain no of iterations or a set of optimal coefficients is obtained.\n",
    "\n",
    "- When the loss function is sufficiently minimised we run our model on the test set by applying a certain threshold value for the sigma(z) probability such as 0.5 so that all the proababilities above 0.5 postive class are treated as belonging to class 1 and probabilities less than or equal to 0.5 as class 0 or negative class.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "# Subtask 2: Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "\n",
    "    def __init__(self, learning_rate = 0.001, iters = 10000):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iters = iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    \n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return np.exp(-np.logaddexp(0, -z))         #this is th same as using 1/(1+e^-z) but i used this as i was facing overflow error in subtask 3\n",
    "    \n",
    "    def fit(self, X, y):                            #   X is a np array of size (no_of_samples * no_of_features)\n",
    "        samples_count, features_count = X.shape\n",
    "        self.weights = np.zeros(features_count)\n",
    "        self.bias = 0\n",
    "\n",
    "        #Below code is used for gradient descent algorithm which is one of the optimisation algorithms other options include newton rhapson method\n",
    "\n",
    "        for iter in range(self.iters):\n",
    "\n",
    "            #for getting z = w0 + w1x1 + w2x2 + w3x3 .... wnxn\n",
    "            linear_eqn = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            predicted = self.sigmoid(linear_eqn)\n",
    "\n",
    "            #gradients\n",
    "            dw = (1 / samples_count) * np.dot(X.T, (predicted - y))\n",
    "            db = (1 / samples_count) * np.sum(predicted - y)\n",
    "\n",
    "            #here bias and weights are updates\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        linear_eqn = np.dot(X, self.weights) + self.bias\n",
    "        predicted = self.sigmoid(linear_eqn)\n",
    "        return (predicted >= 0.5).astype(int)\n",
    "    \n",
    "    def f1_score(self, y, predictions):                 #to be used in subtask 3 for hyper_paramter tuning\n",
    "        true_positives = np.sum((y == 1) & (predictions == 1))\n",
    "        false_positives = np.sum((y == 0) & (predictions == 1))\n",
    "        false_negatives = np.sum((y == 1) & (predictions == 0))\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('ds1_train.csv')\n",
    "X_train = train[['x_1', 'x_2']].to_numpy()\n",
    "y_train = train['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For training data, accuracy =  0.8\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train)\n",
    "accuracy = np.mean(predictions == y_train)\n",
    "print(\"For training data, accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('ds1_test.csv')\n",
    "X_test = test[['x_1', 'x_2']].to_numpy()\n",
    "y_test = test['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test data, accuracy =  0.79\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(\"For test data, accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Subtask 3: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The hyper parameters which affects the logistic regression model's performance are learning rate and no of iterations performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning learning rate and no of iterations to perform using grid search technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible options:  [(0.01, 1000), (0.01, 2000), (0.01, 3000), (0.01, 4000), (0.01, 5000), (0.01, 6000), (0.01, 7000), (0.01, 8000), (0.01, 9000), (0.01, 10000), (0.02, 1000), (0.02, 2000), (0.02, 3000), (0.02, 4000), (0.02, 5000), (0.02, 6000), (0.02, 7000), (0.02, 8000), (0.02, 9000), (0.02, 10000), (0.03, 1000), (0.03, 2000), (0.03, 3000), (0.03, 4000), (0.03, 5000), (0.03, 6000), (0.03, 7000), (0.03, 8000), (0.03, 9000), (0.03, 10000), (0.04, 1000), (0.04, 2000), (0.04, 3000), (0.04, 4000), (0.04, 5000), (0.04, 6000), (0.04, 7000), (0.04, 8000), (0.04, 9000), (0.04, 10000), (0.05, 1000), (0.05, 2000), (0.05, 3000), (0.05, 4000), (0.05, 5000), (0.05, 6000), (0.05, 7000), (0.05, 8000), (0.05, 9000), (0.05, 10000), (0.001, 1000), (0.001, 2000), (0.001, 3000), (0.001, 4000), (0.001, 5000), (0.001, 6000), (0.001, 7000), (0.001, 8000), (0.001, 9000), (0.001, 10000), (0.002, 1000), (0.002, 2000), (0.002, 3000), (0.002, 4000), (0.002, 5000), (0.002, 6000), (0.002, 7000), (0.002, 8000), (0.002, 9000), (0.002, 10000), (0.003, 1000), (0.003, 2000), (0.003, 3000), (0.003, 4000), (0.003, 5000), (0.003, 6000), (0.003, 7000), (0.003, 8000), (0.003, 9000), (0.003, 10000), (0.004, 1000), (0.004, 2000), (0.004, 3000), (0.004, 4000), (0.004, 5000), (0.004, 6000), (0.004, 7000), (0.004, 8000), (0.004, 9000), (0.004, 10000), (0.005, 1000), (0.005, 2000), (0.005, 3000), (0.005, 4000), (0.005, 5000), (0.005, 6000), (0.005, 7000), (0.005, 8000), (0.005, 9000), (0.005, 10000), (0.0001, 1000), (0.0001, 2000), (0.0001, 3000), (0.0001, 4000), (0.0001, 5000), (0.0001, 6000), (0.0001, 7000), (0.0001, 8000), (0.0001, 9000), (0.0001, 10000), (0.0002, 1000), (0.0002, 2000), (0.0002, 3000), (0.0002, 4000), (0.0002, 5000), (0.0002, 6000), (0.0002, 7000), (0.0002, 8000), (0.0002, 9000), (0.0002, 10000), (0.0003, 1000), (0.0003, 2000), (0.0003, 3000), (0.0003, 4000), (0.0003, 5000), (0.0003, 6000), (0.0003, 7000), (0.0003, 8000), (0.0003, 9000), (0.0003, 10000), (0.0004, 1000), (0.0004, 2000), (0.0004, 3000), (0.0004, 4000), (0.0004, 5000), (0.0004, 6000), (0.0004, 7000), (0.0004, 8000), (0.0004, 9000), (0.0004, 10000), (0.0005, 1000), (0.0005, 2000), (0.0005, 3000), (0.0005, 4000), (0.0005, 5000), (0.0005, 6000), (0.0005, 7000), (0.0005, 8000), (0.0005, 9000), (0.0005, 10000)]\n"
     ]
    }
   ],
   "source": [
    "hyper_param = []\n",
    "learning_rates = [0.01, 0.02, 0.03, 0.04, 0.05, 0.001, 0.002, 0.003, 0.004, 0.005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005]\n",
    "iteration_choices = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "for i in learning_rates:\n",
    "    for j in iteration_choices:\n",
    "        hyper_param.append((i, j))\n",
    "\n",
    "print(\"Possible options: \", hyper_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('ds1_train.csv')\n",
    "X_train = train[['x_1', 'x_2']].to_numpy()\n",
    "y_train = train['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('ds1_test.csv')\n",
    "X_test = test[['x_1', 'x_2']].to_numpy()\n",
    "y_test = test['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best possible hyperparameter combination based on Accuracy of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Accuracy  = 0.87 achieved by our model with learning rate 0.003 and no of iterations = 9000\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0\n",
    "best_learning_rate = 0\n",
    "best_iterations = 0\n",
    "\n",
    "for l in range(len(hyper_param)):\n",
    "    model = MyLogisticRegression( learning_rate= hyper_param[l][0], iters = hyper_param[l][1])\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)         #training the model on test data set\n",
    "\n",
    "    curr_accuracy = np.mean(predictions == y_test)          #evaluating the accuracy on test data set\n",
    "    if max_accuracy < curr_accuracy:\n",
    "        max_accuracy = curr_accuracy\n",
    "        best_learning_rate = hyper_param[l][0]\n",
    "        best_iterations = hyper_param[l][1]\n",
    "\n",
    "print(f\"Maximum Accuracy  = {max_accuracy} achieved by our model with learning rate {best_learning_rate} and no of iterations = {best_iterations}\")\n",
    "\n",
    "#the code takes approx a minute to run as i am considering 150 combinations of learning rates and iterations to perform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding best possible hyperparameter combination based on F1 score of the model\n",
    "\n",
    "- F1 score takes into account both precision and recall. Its the harmonic mean of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f1 score  = 0.8659793814432989 achieved by our model with learning rate 0.003 and no of iterations = 9000\n"
     ]
    }
   ],
   "source": [
    "best_f1_score = 0\n",
    "best_learning_rate = 0\n",
    "best_iterations = 0\n",
    "\n",
    "for l in range(len(hyper_param)):\n",
    "    model = MyLogisticRegression( learning_rate= hyper_param[l][0], iters = hyper_param[l][1])\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)         #training the model on test data set\n",
    "\n",
    "    f1_score = model.f1_score(y_test, predictions)          #evaluating the f1_score on test data set\n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        best_learning_rate = hyper_param[l][0]\n",
    "        best_iterations = hyper_param[l][1]\n",
    "        \n",
    "print(f\"Best f1 score  = {best_f1_score} achieved by our model with learning rate {best_learning_rate} and no of iterations = {best_iterations}\")\n",
    "\n",
    "#the code takes approx half a minute to run as i am considering 150 combinations of learning rates and iterations to check which is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the best set of learning rate and no of iterations may differ depending on the type of environment that we choose to work upon for example, i got a different result when i used my code on google collab than when i ran my code locally on vs code on mac\n",
    "\n",
    "- Also, I had avoided considering iterations in 1,00,000 range as i didn't want to increase the load on computer though we can further increase our no of iterations while performing grid search to include a large number of possibilities and get the best fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Subtask 4: Comparison with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('ds1_train.csv')\n",
    "X_train = train[['x_1', 'x_2']].to_numpy()\n",
    "y_train = train['y'].to_numpy()\n",
    "\n",
    "test = pd.read_csv('ds1_test.csv')\n",
    "X_test = test[['x_1', 'x_2']].to_numpy()\n",
    "y_test = test['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "\n",
    "model = MyLogisticRegression(learning_rate=0.003, iters=9000)           #   i have chosen learning_rate as 0.003 and iterations as 9000 \n",
    "model.fit(X_train, y_train)                                             #  as it was the best for test data accuracy and f1 score in VS_Code\n",
    "                                                                        #   alternatively one can input the best_learning_rate and iterations\n",
    "                                                                        #   from the variables that i have provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training DATA SET ds1_train: \n",
      "F1_Score for Scikit-Learn model of logistic regression is  0.8839506172839507\n",
      "Accuracy for Scikit-Learn model of logistic regression is  0.8825\n",
      "\n",
      "F1_Score for My Model of logistic regression is  0.8010973936899863\n",
      "Accuracy for My Model of logistic regression is  0.81875\n"
     ]
    }
   ],
   "source": [
    "predictions_sk_train = logmodel.predict(X_train)\n",
    "predictions_my_train = model.predict(X_train)\n",
    "\n",
    "print(\"On Training DATA SET ds1_train: \")\n",
    "print(\"F1_Score for Scikit-Learn model of logistic regression is \", f1_score(y_train, predictions_sk_train))\n",
    "print(\"Accuracy for Scikit-Learn model of logistic regression is \", accuracy_score(y_train, predictions_sk_train))\n",
    "print('')\n",
    "\n",
    "print(\"F1_Score for My Model of logistic regression is \", model.f1_score(y_train, predictions_my_train))\n",
    "print(\"Accuracy for My Model of logistic regression is \", np.mean(predictions_my_train == y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Test DATA SET ds1_test: \n",
      "F1_Score for Scikit-Learn model of logistic regression is  0.9056603773584904\n",
      "Accuracy for Scikit-Learn model of logistic regression is  0.9\n",
      "\n",
      "F1_Score for Scikit-Learn model of logistic regression is  0.8659793814432989\n",
      "Accuracy for My Model of logistic regression is  0.87\n"
     ]
    }
   ],
   "source": [
    "predictions_sk_test = logmodel.predict(X_test)\n",
    "predictions_my_test = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"On Test DATA SET ds1_test: \")\n",
    "print(\"F1_Score for Scikit-Learn model of logistic regression is \", f1_score(y_test, predictions_sk_test))\n",
    "print(\"Accuracy for Scikit-Learn model of logistic regression is \", accuracy_score(y_test, predictions_sk_test))\n",
    "print('')\n",
    "\n",
    "print(\"F1_Score for Scikit-Learn model of logistic regression is \",model.f1_score(y_test, predictions_my_test ))\n",
    "print(\"Accuracy for My Model of logistic regression is \", np.mean(predictions_my_test == y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
